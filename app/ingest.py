import os
from langchain_community.document_loaders import (
    DirectoryLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredPowerPointLoader,
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

# ë°ì´í„°ê°€ ì €ì¥ë  ê²½ë¡œì™€ ë²¡í„° DBê°€ ì €ì¥ë  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
DATA_PATH = "./data"
DB_PATH = "./db"

def load_documents():
    """data í´ë”ì— ìˆëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    loader = DirectoryLoader(
        DATA_PATH,
        glob="**/*",
        loader_map={
            ".pdf": PyPDFLoader,
            ".docx": UnstructuredWordDocumentLoader,
            ".pptx": UnstructuredPowerPointLoader,
            ".xlsx": UnstructuredExcelLoader,
            ".md": UnstructuredMarkdownLoader,
            ".java": TextLoader,
            ".py": TextLoader, # Python ì†ŒìŠ¤ ì½”ë“œ ë¡œë” ì¶”ê°€
            ".txt": TextLoader,
        },
        show_progress=True,
        use_multithreading=True,
    )
    documents = loader.load()
    return documents

def split_documents(documents):
    """ë¡œë“œëœ ë¬¸ì„œë¥¼ ì–¸ì–´ì— ë§ì¶° ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„(Chunk)ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    chunks = []
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    # Java ì½”ë“œìš© ë¶„í• ê¸°
    java_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.JAVA, chunk_size=1000, chunk_overlap=200
    )
    # Python ì½”ë“œìš© ë¶„í• ê¸° ì¶”ê°€
    python_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON, chunk_size=1000, chunk_overlap=200
    )

    for doc in documents:
        # íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ì ˆí•œ ë¶„í• ê¸°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        file_extension = os.path.splitext(doc.metadata.get("source", ""))[1]
        
        if file_extension == ".java":
            split_chunks = java_splitter.split_documents([doc])
        elif file_extension == ".py":
            split_chunks = python_splitter.split_documents([doc])
        else:
            split_chunks = text_splitter.split_documents([doc])
        
        chunks.extend(split_chunks)
        
    return chunks

def save_to_vector_db(chunks):
    """ë¶„í• ëœ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not chunks:
        print("ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    embedding_function = SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    vector_db = Chroma.from_documents(
        chunks,
        embedding_function,
        persist_directory=DB_PATH
    )
    
    print(f"ì´ {len(chunks)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print("1. ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    documents = load_documents()
    if not documents:
        print("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    print("\n2. ë¬¸ì„œë¥¼ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    chunks = split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    print("\n3. ë¶„í• ëœ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")
    save_to_vector_db(chunks)
    
    print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == '__main__':
    main()
