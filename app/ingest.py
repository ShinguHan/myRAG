import os
from langchain_community.document_loaders import (
    DirectoryLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredPowerPointLoader,
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
import time

# ë°ì´í„°ê°€ ì €ì¥ë  ê²½ë¡œì™€ ë²¡í„° DBê°€ ì €ì¥ë  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
DATA_PATH = "./data"
DB_PATH = "./db"

# --- ì´ í•¨ìˆ˜ë¥¼ í†µì§¸ë¡œ êµì²´í•´ì£¼ì„¸ìš” ---
def load_documents():
    """data í´ë”ì— ìˆëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° ì ì ˆí•œ ë¡œë”ë¥¼ ì‚¬ìš©í•´ ë¡œë“œí•©ë‹ˆë‹¤."""
    documents = []
    # loader_map ëŒ€ì‹ , íŒŒì¼ í™•ì¥ìì™€ ë¡œë” í´ë˜ìŠ¤ë¥¼ ì§ì ‘ ë§¤í•‘í•©ë‹ˆë‹¤.
    ext_to_loader_map = {
        ".pdf": PyPDFLoader,
        ".docx": UnstructuredWordDocumentLoader,
        ".pptx": UnstructuredPowerPointLoader,
        ".xlsx": UnstructuredExcelLoader,
        ".md": UnstructuredMarkdownLoader,
        ".java": TextLoader,
        ".py": TextLoader,
        ".txt": TextLoader,
    }

    # data í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ìˆœíšŒí•©ë‹ˆë‹¤.
    for root, _, files in os.walk(DATA_PATH):
        for file in files:
            file_path = os.path.join(root, file)
            # íŒŒì¼ í™•ì¥ìë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            file_ext = os.path.splitext(file)[1].lower()
            
            # ë§¤í•‘ëœ ë¡œë”ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            if file_ext in ext_to_loader_map:
                loader_class = ext_to_loader_map[file_ext]
                try:
                    # í•´ë‹¹ ë¡œë”ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
                    print(f"Loading {file_path} with {loader_class.__name__}...")
                    loader = loader_class(file_path)
                    loaded_docs = loader.load()
                    documents.extend(loaded_docs)
                except Exception as e:
                    print(f"Error loading file {file_path}: {e}")
            else:
                print(f"Skipping file {file_path} as no loader is available for extension {file_ext}")

    return documents

def split_documents(documents):
    """ë¡œë“œëœ ë¬¸ì„œë¥¼ ì–¸ì–´ì— ë§ì¶° ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„(Chunk)ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    chunks = []
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    # Java ì½”ë“œìš© ë¶„í• ê¸°
    java_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.JAVA, chunk_size=1000, chunk_overlap=200
    )
    # Python ì½”ë“œìš© ë¶„í• ê¸° ì¶”ê°€
    python_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON, chunk_size=1000, chunk_overlap=200
    )

    for doc in documents:
        # íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ì ˆí•œ ë¶„í• ê¸°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        file_extension = os.path.splitext(doc.metadata.get("source", ""))[1]
        
        if file_extension == ".java":
            split_chunks = java_splitter.split_documents([doc])
        elif file_extension == ".py":
            split_chunks = python_splitter.split_documents([doc])
        else:
            split_chunks = text_splitter.split_documents([doc])
        
        chunks.extend(split_chunks)
        
    return chunks

def save_to_vector_db(chunks):
    """ë¶„í• ëœ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not chunks:
        print("ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    embedding_function = SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    vector_db = Chroma.from_documents(
        chunks,
        embedding_function,
        persist_directory=DB_PATH
    )
    
    print(f"ì´ {len(chunks)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    start = time.time()
    print("1. ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    documents = load_documents()
    if not documents:
        print("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    print("\n2. ë¬¸ì„œë¥¼ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    chunks = split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    print("\n3. ë¶„í• ëœ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")
    save_to_vector_db(chunks)
    
    print("\nğŸ‰ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    end = time.time()
    elapsed = end - start
    print(f'ê²½ê³¼ ì‹œê°„ì€ {elapsed}ì´ˆ ì…ë‹ˆë‹¤')
if __name__ == '__main__':
    main()
